# LeetCode Benchmark Workflow
# PRのLeetCodeソリューションをDocker内で実行してベンチマークを取る

name: LeetCode Benchmark

on:
  workflow_dispatch:
    inputs:
      evaluation_id:
        description: 'LeetCode Evaluation ID'
        required: true
        type: string
      language:
        description: 'Programming Language'
        required: true
        type: choice
        options:
          - python
          - javascript
          - typescript
          - java
          - go
          - swift
      code:
        description: 'Base64 encoded user code'
        required: true
        type: string
      test_cases:
        description: 'Base64 encoded test cases JSON'
        required: true
        type: string
      run_count:
        description: 'Number of benchmark runs'
        required: false
        default: '20'
        type: string
      callback_url:
        description: 'Callback URL to send results'
        required: true
        type: string
      solution_index:
        description: 'Solution index (for optimal solution benchmarks)'
        required: false
        default: ''
        type: string

env:
  DOCKER_IMAGE: leetcode-runner
  TIMEOUT_SECONDS: 10
  MEMORY_LIMIT: 512m
  CPU_LIMIT: 1

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        run: |
          docker build -t ${{ env.DOCKER_IMAGE }} .github/leetcode-runner/

      - name: Prepare workspace
        run: |
          mkdir -p workspace
          mkdir -p results
          chmod 777 results

          # Decode code from base64
          echo "${{ inputs.code }}" | base64 -d > workspace/solution.${{ inputs.language == 'python' && 'py' || inputs.language == 'javascript' && 'js' || inputs.language == 'typescript' && 'ts' || inputs.language == 'java' && 'java' || inputs.language == 'go' && 'go' || 'swift' }}

          # Decode test cases from base64
          echo "${{ inputs.test_cases }}" | base64 -d > workspace/test_cases.json

          # Show files for debugging
          echo "=== Solution file ==="
          cat workspace/solution.*
          echo ""
          echo "=== Test cases ==="
          cat workspace/test_cases.json

      - name: Determine file extension
        id: ext
        run: |
          case "${{ inputs.language }}" in
            python) echo "ext=py" >> $GITHUB_OUTPUT ;;
            javascript) echo "ext=js" >> $GITHUB_OUTPUT ;;
            typescript) echo "ext=ts" >> $GITHUB_OUTPUT ;;
            java) echo "ext=java" >> $GITHUB_OUTPUT ;;
            go) echo "ext=go" >> $GITHUB_OUTPUT ;;
            swift) echo "ext=swift" >> $GITHUB_OUTPUT ;;
          esac

      - name: Run benchmark in Docker
        id: benchmark
        run: |
          # Run Docker container with security constraints
          docker run \
            --rm \
            --network=none \
            --memory=${{ env.MEMORY_LIMIT }} \
            --cpus=${{ env.CPU_LIMIT }} \
            --read-only \
            --tmpfs /tmp:rw,noexec,nosuid,size=100m \
            -v "$(pwd)/workspace:/workspace:ro" \
            -v "$(pwd)/results:/results:rw" \
            -v "$(pwd)/.github/leetcode-runner/helpers:/leetcode/helpers:ro" \
            --entrypoint /bin/bash \
            ${{ env.DOCKER_IMAGE }} \
            -c "
              echo 'DEBUG: Starting container'
              cp -r /workspace/* /tmp/ || echo 'Copy failed'
              echo 'DEBUG: Running runner.sh...'
              /leetcode/runner.sh '${{ inputs.language }}' '/tmp/solution.${{ steps.ext.outputs.ext }}' '/tmp/test_cases.json' '/results/result.json' '${{ inputs.run_count }}' '${{ env.TIMEOUT_SECONDS }}' || echo 'Runner failed with exit code:' \$?
            " 2>&1 || true

          # Copy result to expected location
          if [ -f results/result.json ]; then
            cp results/result.json benchmark_result.json
            echo "=== Benchmark Result ==="
            cat benchmark_result.json
          else
            echo '{"success": false, "error": "No result file generated"}' > benchmark_result.json
            echo "=== No result file ==="
          fi

      - name: Send results to callback
        run: |
          # Check if result file exists and is valid JSON
          if [ -f benchmark_result.json ] && jq empty benchmark_result.json 2>/dev/null; then
            RESULT=$(cat benchmark_result.json)
          else
            RESULT='{"success": false, "error": "Benchmark execution failed"}'
          fi

          # Build callback payload
          SOLUTION_INDEX="${{ inputs.solution_index }}"
          if [ -n "$SOLUTION_INDEX" ]; then
            PAYLOAD="{
              \"evaluationId\": \"${{ inputs.evaluation_id }}\",
              \"language\": \"${{ inputs.language }}\",
              \"solutionIndex\": $SOLUTION_INDEX,
              \"result\": $RESULT
            }"
          else
            PAYLOAD="{
              \"evaluationId\": \"${{ inputs.evaluation_id }}\",
              \"language\": \"${{ inputs.language }}\",
              \"result\": $RESULT
            }"
          fi

          # Send result to callback URL
          curl -X POST "${{ inputs.callback_url }}" \
            -H "Content-Type: application/json" \
            -H "X-Evaluation-ID: ${{ inputs.evaluation_id }}" \
            -d "$PAYLOAD"

      - name: Upload benchmark result
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-result-${{ inputs.evaluation_id }}
          path: benchmark_result.json
          retention-days: 7
